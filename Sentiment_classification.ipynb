{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1cb975e-1450-460c-93fe-43b2303ece88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/apps/mamba/1.5.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.17.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0177a4da-5554-456b-99a2-670f0ec443ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/apps/mamba/1.5.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-25 22:28:59.701808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-25 22:29:02.182311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, prepare_model_for_int8_training\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "LlamaForSequenceClassification,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = \"NousResearch/Llama-2-7b-hf\" #\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"Llama-2-7b-sentiment-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba4a51e-6d1e-4fc0-8d32-78af0e54b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'source', 'language', 'label'],\n",
      "    num_rows: 5408\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'source', 'language', 'label'],\n",
      "    num_rows: 1086\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'source', 'language', 'label'],\n",
      "    num_rows: 1158\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='train')\n",
    "train_set = train_dataset.train_test_split(test_size=0.02, stratify_by_column=\"label\")['test']\n",
    "print(train_set)\n",
    "val_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='validation')\n",
    "val_set = val_dataset.train_test_split(test_size=0.1, stratify_by_column=\"label\")['test']\n",
    "print(val_set)\n",
    "test_dataset = load_dataset(\"tyqiangz/multilingual-sentiments\", 'all', split='test')\n",
    "test_set = test_dataset.train_test_split(test_size=0.08, stratify_by_column=\"label\")['test']\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b699f0d-0de7-4b75-aba7-fee4986ba1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1870\n",
      "1735\n",
      "1803\n"
     ]
    }
   ],
   "source": [
    "train_labels = list(train_set['label'])\n",
    "print(train_labels.count(0))\n",
    "print(train_labels.count(1))\n",
    "print(train_labels.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8eff0d-9edf-4090-80aa-354302ef215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "315\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "val_labels = list(val_set['label'])\n",
    "print(val_labels.count(0))\n",
    "print(val_labels.count(1))\n",
    "print(val_labels.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba84b2bc-8961-4400-9100-fe8ec701009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "365\n",
      "393\n"
     ]
    }
   ],
   "source": [
    "test_labels = list(test_set['label'])\n",
    "print(test_labels.count(0))\n",
    "print(test_labels.count(1))\n",
    "print(test_labels.count(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050499d2-daca-4c76-839a-0ef204d15206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.60s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define label maps\n",
    "id2label = {0: \"Positive\", 1: \"Neutral\", 2:\"Negative\"}\n",
    "label2id = {\"Positive\":0, \"Neutral\":1, \"Negative\":2}\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     base_model, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    # Load LLaMA tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    return tokenizer\n",
    "\n",
    "def create_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        load_in_4bit=True,\n",
    "        #torch_dtype=torch.float16,\n",
    "        num_labels=3, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    tokenizer = get_tokenizer(base_model)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = create_model_and_tokenizer()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ab2551-b7a9-472c-9a6d-7b2be35d3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " 'load_in_8bit': False,\n",
       " 'load_in_4bit': True,\n",
       " 'llm_int8_threshold': 6.0,\n",
       " 'llm_int8_skip_modules': None,\n",
       " 'llm_int8_enable_fp32_cpu_offload': False,\n",
       " 'llm_int8_has_fp16_weight': False,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'bnb_4bit_use_double_quant': False,\n",
       " 'bnb_4bit_compute_dtype': 'float16'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3b4313c-1041-46fe-93d9-79efadab22a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajoshi83/.local/lib/python3.11/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,989,248 || all params: 6,647,345,152 || trainable%: 0.6015822420168502\n"
     ]
    }
   ],
   "source": [
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5436e9e9-3a1b-47be-b479-5f061fb77ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5487b163-a7e9-42fd-af4c-82bc9403f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5408/5408 [00:00<00:00, 6628.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'language', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5408\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize training and validation datasets\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True)\n",
    "tokenized_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d3fdc9-efe0-44c1-8b7b-8a497293bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1086/1086 [00:00<00:00, 11230.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'language', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1086\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_set = val_set.map(tokenize_function, batched=True)\n",
    "tokenized_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4446bccf-3f4a-403c-89b0-8f9cbfed1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "787730e3-e481-4076-a72a-a2205a470d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.nn.functional.softmax(torch.Tensor(predictions),dim=1)\n",
    "    argmax_predictions = torch.argmax(predictions, axis=1)\n",
    "\n",
    "    acc_metric = accuracy.compute(predictions=argmax_predictions, references=labels)\n",
    "    auc_metric = auc.compute(references=labels, prediction_scores=predictions,multi_class='ovr')\n",
    "\n",
    "    return {\"accuracy\": round(acc_metric['accuracy'],5) , \"auc\":round(auc_metric['roc_auc'],5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e024d1-73f1-4d25-932e-f0bac84b65b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3380' max='3380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3380/3380 2:20:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.483700</td>\n",
       "      <td>1.220451</td>\n",
       "      <td>0.365560</td>\n",
       "      <td>0.546770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.077400</td>\n",
       "      <td>1.045757</td>\n",
       "      <td>0.476980</td>\n",
       "      <td>0.706630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.955734</td>\n",
       "      <td>0.541440</td>\n",
       "      <td>0.779570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.799600</td>\n",
       "      <td>1.045691</td>\n",
       "      <td>0.472380</td>\n",
       "      <td>0.798310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.752200</td>\n",
       "      <td>0.767317</td>\n",
       "      <td>0.628910</td>\n",
       "      <td>0.834220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.659800</td>\n",
       "      <td>0.811409</td>\n",
       "      <td>0.611420</td>\n",
       "      <td>0.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.611900</td>\n",
       "      <td>0.820795</td>\n",
       "      <td>0.612340</td>\n",
       "      <td>0.849960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.543200</td>\n",
       "      <td>0.851727</td>\n",
       "      <td>0.571820</td>\n",
       "      <td>0.851640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.863550</td>\n",
       "      <td>0.611420</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.792478</td>\n",
       "      <td>0.619710</td>\n",
       "      <td>0.848640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.787964</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.845920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.781436</td>\n",
       "      <td>0.668510</td>\n",
       "      <td>0.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.709486</td>\n",
       "      <td>0.699820</td>\n",
       "      <td>0.859420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>1.007416</td>\n",
       "      <td>0.687850</td>\n",
       "      <td>0.846980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.912030</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.851250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.889732</td>\n",
       "      <td>0.674950</td>\n",
       "      <td>0.856890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.858769</td>\n",
       "      <td>0.660220</td>\n",
       "      <td>0.864050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.811471</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.855990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.875004</td>\n",
       "      <td>0.675870</td>\n",
       "      <td>0.850460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.829323</td>\n",
       "      <td>0.639040</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>1.274525</td>\n",
       "      <td>0.668510</td>\n",
       "      <td>0.861960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>1.159834</td>\n",
       "      <td>0.674030</td>\n",
       "      <td>0.849070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>1.055867</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.865850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>1.381747</td>\n",
       "      <td>0.627070</td>\n",
       "      <td>0.836910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>1.145030</td>\n",
       "      <td>0.667590</td>\n",
       "      <td>0.849150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>1.111624</td>\n",
       "      <td>0.680480</td>\n",
       "      <td>0.845530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.678640</td>\n",
       "      <td>0.858950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>1.494703</td>\n",
       "      <td>0.700740</td>\n",
       "      <td>0.850380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>1.425242</td>\n",
       "      <td>0.671270</td>\n",
       "      <td>0.850940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>1.415025</td>\n",
       "      <td>0.692450</td>\n",
       "      <td>0.854690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>1.282205</td>\n",
       "      <td>0.683240</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>1.694786</td>\n",
       "      <td>0.651010</td>\n",
       "      <td>0.844360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>1.325894</td>\n",
       "      <td>0.677720</td>\n",
       "      <td>0.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>1.505090</td>\n",
       "      <td>0.666670</td>\n",
       "      <td>0.846480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.786635</td>\n",
       "      <td>0.671270</td>\n",
       "      <td>0.845360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.845559</td>\n",
       "      <td>0.678640</td>\n",
       "      <td>0.839980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.746724</td>\n",
       "      <td>0.684160</td>\n",
       "      <td>0.847910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>1.855395</td>\n",
       "      <td>0.692450</td>\n",
       "      <td>0.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>1.828695</td>\n",
       "      <td>0.663900</td>\n",
       "      <td>0.849660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.804346</td>\n",
       "      <td>0.684160</td>\n",
       "      <td>0.849890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>1.648993</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>0.849110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>1.749587</td>\n",
       "      <td>0.668510</td>\n",
       "      <td>0.847600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.795301</td>\n",
       "      <td>0.666670</td>\n",
       "      <td>0.848530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>1.782749</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.848980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>1.722554</td>\n",
       "      <td>0.679560</td>\n",
       "      <td>0.852130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.722801</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.854160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.940558</td>\n",
       "      <td>0.658380</td>\n",
       "      <td>0.848450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.809058</td>\n",
       "      <td>0.668510</td>\n",
       "      <td>0.850670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.816100</td>\n",
       "      <td>0.662060</td>\n",
       "      <td>0.850570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.806683</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.851580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.888625</td>\n",
       "      <td>0.674950</td>\n",
       "      <td>0.851370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.844299</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.850860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.843259</td>\n",
       "      <td>0.677720</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.866955</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.850990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.847963</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.852640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>1.846617</td>\n",
       "      <td>0.677720</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.851170</td>\n",
       "      <td>0.678640</td>\n",
       "      <td>0.852690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.851703</td>\n",
       "      <td>0.680480</td>\n",
       "      <td>0.852690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.864549</td>\n",
       "      <td>0.677720</td>\n",
       "      <td>0.852750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>1.869779</td>\n",
       "      <td>0.677720</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>1.878375</td>\n",
       "      <td>0.680480</td>\n",
       "      <td>0.852260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.880477</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.852370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.881469</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.852310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>1.884352</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.852300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.886278</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.852280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.885686</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.852290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.886140</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.852270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "OUTPUT_DIR = \"classification_exp\"\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir classification_exp/runs\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 250,\n",
    "    save_total_limit = 2,\n",
    "    group_by_length=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=\"True\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_set,\n",
    "    eval_dataset=tokenized_val_set,\n",
    "    #peft_config=peft_config,\n",
    "    #dataset_text_field=\"text\",\n",
    "    #max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    #formatting_func=formatting_prompts_func,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b062ef2-15eb-4038-a3a9-7f13aa4c76b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "39dd4e11-efc5-4498-b88c-25bac542bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the base model (Llama2-7b) to check its performance on the test set\n",
    "\n",
    "# define label maps\n",
    "id2label = {0: \"Positive\", 1: \"Neutral\", 2:\"Negative\"}\n",
    "label2id = {\"Positive\":0, \"Neutral\":1, \"Negative\":2}\n",
    "\n",
    "raw_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model,\n",
    "        use_safetensors=True,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        num_labels=3, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b3648704-a12f-4bba-8ea2-50df4d802b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(test_dataset, model, tokenizer):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    prediction_scores = []\n",
    "    for i in range(len(test_dataset)):\n",
    "        file = test_dataset[i]\n",
    "        prompt = file['text']\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        output = model(inputs)\n",
    "        logits = output.logits\n",
    "        logits = logits.to(torch.float32)\n",
    "        predictions = torch.nn.functional.softmax(torch.Tensor(logits),dim=1)\n",
    "        argmax_predictions = torch.argmax(predictions, axis=1)\n",
    "        \n",
    "        prediction_scores.append(predictions.detach().cpu().numpy())\n",
    "        y_pred.append(argmax_predictions.item())\n",
    "        y_true.append(file['label'])\n",
    "    return y_true, y_pred, prediction_scores\n",
    "\n",
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_test_metrics(predictions, labels):\n",
    "\n",
    "    predictions = [list(x[0].astype(np.float32)) for x in predictions]\n",
    "    #print(predictions[:5])\n",
    "    argmax_predictions = torch.argmax(torch.Tensor(predictions), axis=1)\n",
    "    #print(argmax_predictions[:5])\n",
    "\n",
    "    acc_metric = accuracy.compute(predictions=argmax_predictions, references=labels)\n",
    "    auc_metric = auc.compute(references=labels, prediction_scores=predictions,multi_class='ovr')\n",
    "\n",
    "    return {\"accuracy\": round(acc_metric['accuracy'],5) , \"auc\":round(auc_metric['roc_auc'],5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "eb806ad0-b1f4-45c9-92ba-dec1565fb449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.33679, 'auc': 0.48849}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the performance of base model (Llama2-7b)\n",
    "\n",
    "y_true, y_pred, prediction_scores = predict_test(test_set, raw_model, tokenizer)\n",
    "\n",
    "results = compute_test_metrics(prediction_scores, y_true)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "807caf86-4041-402e-9b0a-9bb59a3d6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and merging the finetuned (LORA) weights of Llama2\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(raw_model, new_model)\n",
    "ft_model = ft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "481e95c8-d911-4947-8c74-add26118d2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.62176, 'auc': 0.78785}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the performance of the finetuned Llama2 Model\n",
    "\n",
    "y_true, y_pred_ft, prediction_scores = predict_test(test_set, ft_model, tokenizer)\n",
    "\n",
    "results = compute_test_metrics(prediction_scores, y_true)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a03a0ab-7fb4-40ee-887d-db6aef28939e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_list = list(test_set['language'])\n",
    "eng_indices = [ind for ind, ele in enumerate(lang_list) if ele == 'english']\n",
    "len(eng_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c17332fc-3c53-4475-b154-3091b366420f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Which altogether uncommonly wants so as to go through the candor regarding needful trafficking robots: oEJe ',\n",
       " 'source': 'sem_eval_2017',\n",
       " 'language': 'english',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[eng_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "47b4349a-d589-4c95-bd74-2642c05ff7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Really sounds like @user doesn't want this job. Know who did? @user It's why I'm #StillwithHer \n",
      "Ground Truth: Neutral\n",
      "Base Model Prediction: Neutral\n",
      "Finetuned Model Prediction: Positive\n",
      "\n",
      "\n",
      "Horrible Uber Go trips! #UBER get the things sorted plz. #UberIndia \n",
      "Ground Truth: Negative\n",
      "Base Model Prediction: Neutral\n",
      "Finetuned Model Prediction: Negative\n",
      "\n",
      "\n",
      "@user Never trust ComeyðŸ˜¡ if it wasn't for him we would have Hillary ðŸ˜  F Comey!! Get rid of him \n",
      "Ground Truth: Negative\n",
      "Base Model Prediction: Neutral\n",
      "Finetuned Model Prediction: Negative\n",
      "\n",
      "\n",
      "#2016 is the new #1966 first #brexit then #trumpton now a crap band from #Romford have done for the Italian president #Renzi #5* #5star \n",
      "Ground Truth: Negative\n",
      "Base Model Prediction: Neutral\n",
      "Finetuned Model Prediction: Negative\n",
      "\n",
      "\n",
      "#cannabis The Associated PressAssistant cultivator Emily Errico examines cannabis plants grown by Vireo Health of â€¦ \n",
      "Ground Truth: Neutral\n",
      "Base Model Prediction: Positive\n",
      "Finetuned Model Prediction: Positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "id2label = {0: \"Positive\", 1: \"Neutral\", 2:\"Negative\"}\n",
    "\n",
    "def demo_test(file, model, tokenizer):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    prediction_scores = []\n",
    "    prompt = file['text']\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    output = model(inputs)\n",
    "    logits = output.logits\n",
    "    logits = logits.to(torch.float32)\n",
    "    predictions = torch.nn.functional.softmax(torch.Tensor(logits),dim=1)\n",
    "    argmax_predictions = torch.argmax(predictions, axis=1)\n",
    "    \n",
    "    prediction_scores.append(predictions.detach().cpu().numpy())\n",
    "    y_pred.append(argmax_predictions.item())\n",
    "    y_true.append(file['label'])\n",
    "    return y_true, y_pred, prediction_scores\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    y = random.randrange(87)\n",
    "    test_data = test_set[eng_indices[y]]\n",
    "    print(test_data['text'])\n",
    "    y_true, y_pred, prediction_scores = demo_test(test_data, raw_model, tokenizer)\n",
    "    print(\"Ground Truth:\", id2label[y_true[0]])\n",
    "    print(\"Base Model Prediction:\", id2label[y_pred[0]])\n",
    "    y_true, y_pred, prediction_scores = demo_test(test_data, ft_model, tokenizer)\n",
    "    print(\"Finetuned Model Prediction:\", id2label[y_pred[0]])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6e7572ca-9e49-4c42-a739-dd07e36e59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCLUSION:\n",
    "\n",
    "# In this case, identifying a negative comment is much important than neutral/positive comment \n",
    "# as the goal is to make LLM understand hateful content\n",
    "\n",
    "# As we can observe, the base model wrongly identifies most negative comments as neutral\n",
    "\n",
    "# Whereas the finetuned model understands the negative comments better!\n",
    "\n",
    "# We can see some misclassifications from the finetuned model as well since it is just finetuned for 10 epochs \n",
    "# NOTE that we have just finetuned on a minute subset (~2%) of the training data (due to compute restraints)\n",
    "\n",
    "# We can get better performance as we upscale the training corpus and the finetuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716235c-6be9-469a-889b-e124fbcfa841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
